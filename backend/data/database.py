from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_text_splitters import CharacterTextSplitter
from langchain_core.documents import Document
import os
import re
import logging
import json
from datetime import datetime
from ..utils.config import CHROMA_DIR, DOCS_DIR

logger = logging.getLogger(__name__)

def get_vectordb():
    try:
        embedding = OpenAIEmbeddings()
        vectordb = Chroma(persist_directory=CHROMA_DIR, embedding_function=embedding)
        
        # effort_estimations.jsonì´ ë²¡í„° DBì— ìˆëŠ”ì§€ í™•ì¸
        try:
            collection = vectordb.get()
            has_effort_data = False
            
            if collection and "metadatas" in collection:
                for metadata in collection["metadatas"]:
                    if isinstance(metadata, dict) and metadata.get("source") == "effort_estimations.json":
                        has_effort_data = True
                        break
            
            # effort_estimations.jsonì´ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ì¸ë±ì‹±
            if not has_effort_data:
                json_file_path = os.path.join(DOCS_DIR, "effort_estimations.json")
                if os.path.exists(json_file_path):
                    logger.info("ğŸ”„ effort_estimations.json ìë™ ì¸ë±ì‹± ì‹œì‘")
                    try:
                        if index_json_data(json_file_path, force=True):
                            logger.info("âœ… effort_estimations.json ìë™ ì¸ë±ì‹± ì™„ë£Œ")
                        else:
                            logger.error("âŒ effort_estimations.json ìë™ ì¸ë±ì‹± ì‹¤íŒ¨")
                    except Exception as idx_error:
                        logger.error(f"âŒ effort_estimations.json ìë™ ì¸ë±ì‹± ì¤‘ ì˜¤ë¥˜: {idx_error}")
        except Exception as coll_error:
            logger.warning(f"âš ï¸ ë²¡í„° DB ì»¬ë ‰ì…˜ í™•ì¸ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œí•˜ê³  ê³„ì†): {coll_error}")
        
        return vectordb
    except Exception as e:
        logger.error(f"âŒ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None

def get_file_metadata(file_path: str):
    """Get file metadata including last modification time"""
    return {
        "source": os.path.basename(file_path),
        "last_modified": datetime.fromtimestamp(os.path.getmtime(file_path)).isoformat(),
        "file_size": os.path.getsize(file_path)
    }

def is_file_modified(file_path: str, vectordb) -> bool:
    """Check if file needs to be reindexed by comparing modification times"""
    try:
        current_metadata = get_file_metadata(file_path)
        collection = vectordb.get()
        
        for metadata in collection["metadatas"]:
            if (isinstance(metadata, dict) and 
                metadata.get("source") == current_metadata["source"]):
                # If file exists in index, check if it's been modified
                if (metadata.get("last_modified") == current_metadata["last_modified"] and
                    metadata.get("file_size") == current_metadata["file_size"]):
                    return False
                return True
        
        # File not found in index
        return True
    except Exception as e:
        logger.error(f"Error checking file modification: {str(e)}")
        return True

def index_document(file_path: str, file_type: str = "pdf", force: bool = False):
    try:
        vectordb = get_vectordb()
        
        # Skip if file is already indexed and hasn't been modified
        if not force and not is_file_modified(file_path, vectordb):
            logger.info(f"ğŸ“ Skipping unchanged file: {file_path}")
            return True

        # Remove existing documents for this file if any
        collection = vectordb.get()
        docs_to_remove = []
        for i, metadata in enumerate(collection["metadatas"]):
            if isinstance(metadata, dict) and metadata.get("source") == os.path.basename(file_path):
                docs_to_remove.append(collection["ids"][i])
        
        if docs_to_remove:
            vectordb._collection.delete(docs_to_remove)
            logger.info(f"ğŸ—‘ï¸ Removed old version of: {file_path}")

        # Load and process the document
        if file_type == "pdf":
            loader = PyMuPDFLoader(file_path)
            documents = loader.load()
        else:  # txt
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
            documents = [Document(page_content=content)]

        # Add metadata to all documents
        file_metadata = get_file_metadata(file_path)
        for doc in documents:
            doc.metadata.update(file_metadata)

        # effort_estimations.txtì˜ ê²½ìš° í‹°ì¼“ë³„ë¡œ ë¶„í• 
        if os.path.basename(file_path) == "effort_estimations.txt":
            # í‹°ì¼“ë³„ë¡œ ë¶„í• 
            docs = []
            content = documents[0].page_content
            tickets = content.split('---\n\n')
            
            for i, ticket in enumerate(tickets):
                if ticket.strip():  # ë¹ˆ í‹°ì¼“ ì œì™¸
                    # ê° í‹°ì¼“ì„ ë³„ë„ ë¬¸ì„œë¡œ ìƒì„±
                    doc = Document(
                        page_content=ticket.strip(),
                        metadata=documents[0].metadata.copy()
                    )
                    doc.metadata["ticket_index"] = i
                    docs.append(doc)
        else:
            # ë‹¤ë¥¸ íŒŒì¼ì€ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
            splitter = CharacterTextSplitter(chunk_size=1200, chunk_overlap=120)
            docs = splitter.split_documents(documents)

        # âœ… ì „ì²˜ë¦¬ í•¨ìˆ˜ ì ìš©
        for idx, doc in enumerate(docs):
            doc.page_content = doc.page_content  # <-- ì´ ë¶€ë¶„!
            doc.metadata["chunk_index"] = idx

        logger.info(f"ğŸ“Š ì´ {len(docs)}ê°œ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤")
        
        # Process documents in smaller batches (ë” ì‘ì€ ë°°ì¹˜ í¬ê¸° ì‚¬ìš©)
        BATCH_SIZE = 10  # 100ì—ì„œ 10ìœ¼ë¡œ ì¤„ì„
        total_added = 0
        for i in range(0, len(docs), BATCH_SIZE):
            batch = docs[i:i + BATCH_SIZE]
            vectordb.add_documents(batch)
            vectordb.persist()
            total_added += len(batch)
            logger.info(f"âœ… Processed batch {i//BATCH_SIZE + 1} of {(len(docs)-1)//BATCH_SIZE + 1} (ì´ {total_added}ê°œ ë¬¸ì„œ ì¶”ê°€ë¨)")
        
        logger.info(f"âœ… Document indexed successfully: {file_path} (ì´ {total_added}ê°œ ë¬¸ì„œ ì €ì¥ë¨)")
        return True
    except Exception as e:
        logger.error(f"âŒ Error indexing document: {str(e)}")
        return False

def get_indexed_files():
    try:
        vectordb = get_vectordb()
        collection = vectordb.get()
        sources = set()
        
        for metadata in collection["metadatas"]:
            if isinstance(metadata, dict) and "source" in metadata:
                sources.add(metadata["source"])
                
        return list(sources)
    except Exception as e:
        logger.error(f"âŒ Error getting indexed files: {str(e)}")
        return []

def remove_document(file_path: str):
    """Remove document from Chroma DB and delete the file"""
    try:
        vectordb = get_vectordb()
        filename = os.path.basename(file_path)
        
        # Remove from Chroma DB
        collection = vectordb.get()
        docs_to_remove = []
        for i, metadata in enumerate(collection["metadatas"]):
            if isinstance(metadata, dict) and metadata.get("source") == filename:
                docs_to_remove.append(collection["ids"][i])
        
        if docs_to_remove:
            vectordb._collection.delete(docs_to_remove)
            vectordb.persist()
            logger.info(f"ğŸ—‘ï¸ Removed document from Chroma DB: {filename}")
        
        # Delete the file
        if os.path.exists(file_path):
            os.remove(file_path)
            logger.info(f"ğŸ—‘ï¸ Deleted file: {file_path}")
            return True
        else:
            logger.warning(f"âš ï¸ File not found: {file_path}")
            return False
            
    except Exception as e:
        logger.error(f"âŒ Error removing document: {str(e)}")
        return False

def reset_vectordb():
    """
    âœ… Chroma DBì˜ ëª¨ë“  ë¬¸ì„œë¥¼ ì•ˆì „í•˜ê²Œ ì œê±°í•©ë‹ˆë‹¤.
    âœ… embedding í˜¸ì¶œ ì—†ì´, ë‹¨ìˆœíˆ ì €ì¥ëœ ë¬¸ì„œ ID ê¸°ì¤€ìœ¼ë¡œ ì‚­ì œí•©ë‹ˆë‹¤.
    """
    try:
        vectordb = get_vectordb()
        collection = vectordb.get()

        all_ids = collection.get("ids", [])
        if all_ids:
            BATCH_SIZE = 100  # ì•ˆì „ì„ ìœ„í•´ ì‚­ì œë„ batch ì²˜ë¦¬ ê°€ëŠ¥
            for i in range(0, len(all_ids), BATCH_SIZE):
                batch_ids = all_ids[i:i + BATCH_SIZE]
                vectordb._collection.delete(batch_ids)
            vectordb.persist()
            logger.info(f"âœ… Successfully reset Chroma DB - {len(all_ids)}ê°œ ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ")
        else:
            logger.info("â„¹ï¸ Chroma DBì— ì‚­ì œí•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return True

    except Exception as e:
        logger.error(f"âŒ Error resetting Chroma DB: {str(e)}")
        return False

def clean_chunk_text(text: str) -> str:
    text = re.sub(r"[^\w\sê°€-í£.,:;!?()\\[\\]<>/@&%\"'\-]", "", text)
    text = re.sub(r"[ \t]+$", "", text, flags=re.MULTILINE)
    text = re.sub(r"\n{2,}", "\n", text)
    text = re.sub(r"[ \t]{2,}", " ", text)

    # âœ… í˜ì´ì§€ ë²ˆí˜¸ ì œê±° (ì˜ˆ: "Page 1", "1 / 27", "15í˜ì´ì§€" ë“±)
    text = re.sub(r"^\s*\d+\s*/\s*\d+\s*$", "", text, flags=re.MULTILINE)
    text = re.sub(r"^\s*Page\s+\d+\s*$", "", text, flags=re.IGNORECASE | re.MULTILINE)
    text = re.sub(r"\d+í˜ì´ì§€", "", text)
    return text.strip()

def save_feedback_to_file(feedback_data):
    """í”¼ë“œë°± ë°ì´í„°ë¥¼ íŒŒì¼ì— ì €ì¥ (ê¸ì •/ë¶€ì • ëª¨ë‘ ì§€ì›)
    
    ì¤‘ë³µ ì²´í¬ ë¡œì§:
    - ì§ˆë¬¸-ë‹µë³€ í•´ì‹œë¡œ ë™ì¼ ì„¸íŠ¸ íŒë³„
    - ê°™ì€ ì§ˆë¬¸-ë‹µë³€ ì„¸íŠ¸ë©´ í”¼ë“œë°± ì¹´ìš´íŠ¸ë§Œ ì¦ê°€
    - ê°™ì€ ì§ˆë¬¸ì´ì§€ë§Œ ë‹µë³€ì´ ë‹¤ë¥´ë©´ ìµœì‹  ë‹µë³€ìœ¼ë¡œ ì—…ë°ì´íŠ¸
    
    Returns:
        dict: {"saved": bool, "is_new": bool, "feedback_count": int}
            - saved: ì €ì¥ ì„±ê³µ ì—¬ë¶€
            - is_new: ìƒˆë¡œìš´ ì§ˆë¬¸-ë‹µë³€ ì„¸íŠ¸ì¸ì§€
            - feedback_count: í•´ë‹¹ ì§ˆë¬¸-ë‹µë³€ ì„¸íŠ¸ì˜ ì´ í”¼ë“œë°± ìˆ˜
    """
    import hashlib
    
    try:
        feedback_type = feedback_data.get("feedback_type", "positive")
        
        # í”¼ë“œë°± íƒ€ì…ì— ë”°ë¼ íŒŒì¼ ì„ íƒ
        if feedback_type == "positive":
            feedback_file = os.path.join(DOCS_DIR, "positive_feedback.json")
            opposite_file = os.path.join(DOCS_DIR, "negative_feedback.json")
        else:
            feedback_file = os.path.join(DOCS_DIR, "negative_feedback.json")
            opposite_file = os.path.join(DOCS_DIR, "positive_feedback.json")
        
        # DOCS_DIR ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
        os.makedirs(DOCS_DIR, exist_ok=True)
        logger.info(f"ğŸ“ DOCS_DIR í™•ì¸: {DOCS_DIR} (ì¡´ì¬: {os.path.exists(DOCS_DIR)})")
        
        # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ (íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì‹œì‘)
        logger.info(f"ğŸ“„ í”¼ë“œë°± íŒŒì¼ í™•ì¸: {feedback_file} (ì¡´ì¬: {os.path.exists(feedback_file)})")
        if os.path.exists(feedback_file):
            try:
                with open(feedback_file, 'r', encoding='utf-8') as f:
                    feedbacks = json.load(f)
                # íŒŒì¼ì´ ë¹„ì–´ìˆê±°ë‚˜ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”
                if not isinstance(feedbacks, list):
                    logger.warning(f"âš ï¸ í”¼ë“œë°± íŒŒì¼ì´ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì´ ì•„ë‹˜, ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”")
                    feedbacks = []
                logger.info(f"ğŸ“Š ê¸°ì¡´ í”¼ë“œë°± ë¡œë“œ: {len(feedbacks)}ê°œ")
            except (json.JSONDecodeError, Exception) as e:
                logger.warning(f"âš ï¸ í”¼ë“œë°± íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}, ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì‹œì‘")
                feedbacks = []
        else:
            feedbacks = []
            logger.info(f"ğŸ“ ìƒˆë¡œìš´ í”¼ë“œë°± íŒŒì¼ ìƒì„± ì˜ˆì •: {feedback_file}")
        
        question = feedback_data["question"]
        answer = feedback_data.get("answer", "")
        
        # ì§ˆë¬¸-ë‹µë³€ í•´ì‹œ ìƒì„± (ì¤‘ë³µ ì²´í¬ìš©)
        qa_hash = hashlib.md5(f"{question}|||{answer}".encode('utf-8')).hexdigest()
        
        # ë¶€ì • í”¼ë“œë°± ì €ì¥ ì‹œ: í•­ìƒ ê¸ì • í”¼ë“œë°± íŒŒì¼ í™•ì¸ ë° ì œê±° (opposite_indexì— ì˜ì¡´í•˜ì§€ ì•ŠìŒ)
        if feedback_type == "negative":
            positive_file_path = os.path.join(DOCS_DIR, "positive_feedback.json")
            if os.path.exists(positive_file_path):
                try:
                    with open(positive_file_path, 'r', encoding='utf-8') as f:
                        positive_feedbacks = json.load(f)
                    
                    # qa_hashë¡œ ì§ì ‘ ê²€ìƒ‰í•˜ì—¬ ì œê±°
                    positive_removed = False
                    positive_removed_feedback = None
                    for i, existing in enumerate(positive_feedbacks):
                        existing_qa_hash = existing.get("qa_hash")
                        if existing_qa_hash == qa_hash:
                            positive_removed_feedback = positive_feedbacks.pop(i)
                            positive_removed = True
                            logger.info(f"ğŸ—‘ï¸ ë¶€ì • í”¼ë“œë°± ì €ì¥: ê¸ì • í”¼ë“œë°± íŒŒì¼ì—ì„œ ì œê±° - {question[:30]}...")
                            break
                    
                    # ì œê±°ëœ í•­ëª©ì´ ìˆìœ¼ë©´ íŒŒì¼ ì €ì¥
                    if positive_removed:
                        with open(positive_file_path, 'w', encoding='utf-8') as f:
                            json.dump(positive_feedbacks, f, ensure_ascii=False, indent=2)
                        logger.info(f"âœ… ê¸ì • í”¼ë“œë°± íŒŒì¼ì—ì„œ ì œê±° ì™„ë£Œ: {positive_file_path}")
                        
                        # ë²¡í„° DBì—ì„œë„ ì œê±°
                        try:
                            feedback_vectordb = get_feedback_vectordb()
                            if feedback_vectordb:
                                collection = feedback_vectordb.get()
                                if collection and "ids" in collection and "metadatas" in collection:
                                    ids_to_remove = []
                                    documents = collection.get("documents", [])
                                    for i, metadata in enumerate(collection["metadatas"]):
                                        if isinstance(metadata, dict) and metadata.get("source") == "positive_feedback":
                                            metadata_question = documents[i] if i < len(documents) else ""
                                            metadata_answer = metadata.get("answer", "")
                                            metadata_qa_hash = hashlib.md5(f"{metadata_question}|||{metadata_answer}".encode('utf-8')).hexdigest()
                                            if metadata_qa_hash == qa_hash and i < len(collection["ids"]):
                                                ids_to_remove.append(collection["ids"][i])
                                    
                                    if ids_to_remove:
                                        feedback_vectordb._collection.delete(ids_to_remove)
                                        try:
                                            feedback_vectordb.persist()
                                        except Exception:
                                            pass
                                        logger.info(f"ğŸ—‘ï¸ ë²¡í„° DBì—ì„œ í”¼ë“œë°± ì œê±°: {len(ids_to_remove)}ê°œ")
                        except Exception as del_error:
                            logger.warning(f"âš ï¸ ë²¡í„° DBì—ì„œ í”¼ë“œë°± ì œê±° ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œí•˜ê³  ê³„ì†): {del_error}")
                except Exception as e:
                    logger.warning(f"âš ï¸ ê¸ì • í”¼ë“œë°± íŒŒì¼ í™•ì¸ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œí•˜ê³  ê³„ì†): {e}")
        
        # ë°˜ëŒ€ íƒ€ì… íŒŒì¼ì—ì„œ ê°™ì€ ì§ˆë¬¸-ë‹µë³€ ì„¸íŠ¸ ì°¾ê¸° (í”¼ë“œë°± íƒ€ì… ë³€ê²½ ì²˜ë¦¬)
        opposite_feedbacks = []
        opposite_index = None
        if os.path.exists(opposite_file):
            try:
                with open(opposite_file, 'r', encoding='utf-8') as f:
                    opposite_feedbacks = json.load(f)
                
                for i, existing in enumerate(opposite_feedbacks):
                    existing_qa_hash = existing.get("qa_hash")
                    if existing_qa_hash == qa_hash:
                        opposite_index = i
                        break
            except Exception as e:
                logger.warning(f"âš ï¸ ë°˜ëŒ€ íƒ€ì… í”¼ë“œë°± íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
        
        # ë°˜ëŒ€ íƒ€ì…ì— ê°™ì€ ì§ˆë¬¸-ë‹µë³€ ì„¸íŠ¸ê°€ ìˆìœ¼ë©´ ì œê±° (í”¼ë“œë°± íƒ€ì… ë³€ê²½)
        if opposite_index is not None:
            opposite_feedback = opposite_feedbacks[opposite_index]
            logger.info(f"ğŸ”„ í”¼ë“œë°± íƒ€ì… ë³€ê²½: {question[:30]}... ({'ê¸ì •' if feedback_type == 'positive' else 'ë¶€ì •'}ìœ¼ë¡œ ë³€ê²½)")
            
            # ë°˜ëŒ€ íƒ€ì… íŒŒì¼ì—ì„œ ì œê±°
            opposite_feedbacks.pop(opposite_index)
            try:
                # DOCS_DIR ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
                os.makedirs(DOCS_DIR, exist_ok=True)
                with open(opposite_file, 'w', encoding='utf-8') as f:
                    json.dump(opposite_feedbacks, f, ensure_ascii=False, indent=2)
                logger.info(f"âœ… ë°˜ëŒ€ íƒ€ì… í”¼ë“œë°± íŒŒì¼ ì €ì¥ ì™„ë£Œ: {opposite_file}")
            except Exception as save_error:
                logger.error(f"âŒ ë°˜ëŒ€ íƒ€ì… í”¼ë“œë°± íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {save_error}")
                raise
            
            # ê¸°ì¡´ í”¼ë“œë°± ì •ë³´ ë³´ì¡´ (ì¹´ìš´íŠ¸, ì‚¬ìš©ì ë“±)
            feedback_count = opposite_feedback.get("feedback_count", 1)
            feedback_users = opposite_feedback.get("feedback_users", [])
            
            # ìƒˆ íƒ€ì…ìœ¼ë¡œ ì €ì¥ (ê¸°ì¡´ ì •ë³´ ë³´ì¡´)
            feedback_data["qa_hash"] = qa_hash
            feedback_data["feedback_count"] = feedback_count
            feedback_data["first_feedback_time"] = opposite_feedback.get("first_feedback_time", opposite_feedback.get("timestamp"))
            feedback_data["last_feedback_time"] = feedback_data.get("timestamp", datetime.now().isoformat())
            feedback_data["feedback_users"] = feedback_users
            
            # ì‚¬ìš©ì ì •ë³´ ì¶”ê°€
            user = feedback_data.get("user")
            if user and user not in feedback_users:
                feedback_data["feedback_users"].append(user)
            
            feedbacks.append(feedback_data)
            
            # íŒŒì¼ì— ì €ì¥
            try:
                with open(feedback_file, 'w', encoding='utf-8') as f:
                    json.dump(feedbacks, f, ensure_ascii=False, indent=2)
                logger.info(f"âœ… í”¼ë“œë°± íŒŒì¼ ì €ì¥ ì™„ë£Œ: {feedback_file}")
            except Exception as save_error:
                logger.error(f"âŒ í”¼ë“œë°± íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {save_error}")
                raise  # ì €ì¥ ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œ ìƒìœ„ì—ì„œ ì²˜ë¦¬
            
            # ê¸ì • í”¼ë“œë°±ìœ¼ë¡œ ë³€ê²½ëœ ê²½ìš° ë²¡í„° DB ì¬ì¸ë±ì‹±
            if feedback_type == "positive":
                index_feedback_data(feedback_file)
            # ë¶€ì • í”¼ë“œë°±ìœ¼ë¡œ ë³€ê²½ëœ ê²½ìš° ë²¡í„° DBì—ì„œ ì œê±° (ê¸ì • í”¼ë“œë°±ë§Œ ì¸ë±ì‹±í•˜ë¯€ë¡œ)
            else:
                # ê¸ì • í”¼ë“œë°± ë²¡í„° DBì—ì„œ í•´ë‹¹ í•­ëª© ì œê±°
                try:
                    feedback_vectordb = get_feedback_vectordb()
                    if feedback_vectordb:
                        collection = feedback_vectordb.get()
                        if collection and "ids" in collection and "metadatas" in collection:
                            ids_to_remove = []
                            documents = collection.get("documents", [])
                            for i, metadata in enumerate(collection["metadatas"]):
                                if isinstance(metadata, dict) and metadata.get("source") == "positive_feedback":
                                    # ë²¡í„° DBì—ëŠ” ì§ˆë¬¸ì´ documents[i]ë¡œ, ë‹µë³€ì´ metadata["answer"]ë¡œ ì €ì¥ë¨
                                    metadata_question = documents[i] if i < len(documents) else ""
                                    metadata_answer = metadata.get("answer", "")
                                    metadata_qa_hash = hashlib.md5(f"{metadata_question}|||{metadata_answer}".encode('utf-8')).hexdigest()
                                    if metadata_qa_hash == qa_hash and i < len(collection["ids"]):
                                        ids_to_remove.append(collection["ids"][i])
                            
                            if ids_to_remove:
                                feedback_vectordb._collection.delete(ids_to_remove)
                                try:
                                    feedback_vectordb.persist()
                                except Exception:
                                    pass
                                logger.info(f"ğŸ—‘ï¸ ë²¡í„° DBì—ì„œ í”¼ë“œë°± ì œê±°: {len(ids_to_remove)}ê°œ")
                except Exception as del_error:
                    logger.warning(f"âš ï¸ ë²¡í„° DBì—ì„œ í”¼ë“œë°± ì œê±° ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œí•˜ê³  ê³„ì†): {del_error}")
            
            return {
                "saved": True,
                "is_new": False,
                "feedback_count": feedback_count,
                "type_changed": True
            }
        
        # ê¸°ì¡´ í”¼ë“œë°±ì—ì„œ ë™ì¼í•œ ì§ˆë¬¸-ë‹µë³€ ì„¸íŠ¸ ì°¾ê¸°
        existing_index = None
        same_question_index = None
        
        for i, existing in enumerate(feedbacks):
            existing_qa_hash = existing.get("qa_hash")
            if existing_qa_hash == qa_hash:
                existing_index = i
                break
            # ê°™ì€ ì§ˆë¬¸ì´ì§€ë§Œ ë‹µë³€ì´ ë‹¤ë¥¸ ê²½ìš°ë„ ì¶”ì  (ìµœì‹  ë‹µë³€ìœ¼ë¡œ ì—…ë°ì´íŠ¸ìš©)
            if existing.get("question") == question and same_question_index is None:
                same_question_index = i
        
        if existing_index is not None:
            # ë™ì¼í•œ ì§ˆë¬¸-ë‹µë³€ ì„¸íŠ¸ê°€ ì´ë¯¸ ì¡´ì¬ â†’ í”¼ë“œë°± ì¹´ìš´íŠ¸ë§Œ ì¦ê°€
            existing_feedback = feedbacks[existing_index]
            existing_feedback["feedback_count"] = existing_feedback.get("feedback_count", 1) + 1
            existing_feedback["last_feedback_time"] = feedback_data.get("timestamp", datetime.now().isoformat())
            existing_feedback["feedback_users"] = existing_feedback.get("feedback_users", [])
            
            # ì‚¬ìš©ì ì •ë³´ ì¶”ê°€ (ì¤‘ë³µ ì œê±°)
            user = feedback_data.get("user")
            if user and user not in existing_feedback["feedback_users"]:
                existing_feedback["feedback_users"].append(user)
            
            logger.info(f"ğŸ“Š í”¼ë“œë°± ì¹´ìš´íŠ¸ ì¦ê°€: {question[:30]}... (ì´ {existing_feedback['feedback_count']}íšŒ)")
            
            # íŒŒì¼ì— ì €ì¥
            try:
                with open(feedback_file, 'w', encoding='utf-8') as f:
                    json.dump(feedbacks, f, ensure_ascii=False, indent=2)
                logger.info(f"âœ… í”¼ë“œë°± íŒŒì¼ ì €ì¥ ì™„ë£Œ: {feedback_file}")
            except Exception as save_error:
                logger.error(f"âŒ í”¼ë“œë°± íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {save_error}")
                raise  # ì €ì¥ ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œ ìƒìœ„ì—ì„œ ì²˜ë¦¬
            
            # ê¸ì • í”¼ë“œë°±ë§Œ ë²¡í„° DB ì¬ì¸ë±ì‹± (ì¹´ìš´íŠ¸ ì¦ê°€í•´ë„ ì¸ë±ì‹±ì€ í•„ìš” ì—†ì„ ìˆ˜ ìˆì§€ë§Œ, ì¼ê´€ì„± ìœ ì§€)
            if feedback_type == "positive":
                index_feedback_data(feedback_file)
            
            return {
                "saved": True,
                "is_new": False,
                "feedback_count": existing_feedback["feedback_count"]
            }
        
        elif same_question_index is not None:
            # ê°™ì€ ì§ˆë¬¸ì´ì§€ë§Œ ë‹µë³€ì´ ë‹¤ë¦„ â†’ ìµœì‹  ë‹µë³€ìœ¼ë¡œ ì—…ë°ì´íŠ¸
            logger.info(f"ğŸ”„ ê°™ì€ ì§ˆë¬¸ì˜ ìµœì‹  ë‹µë³€ìœ¼ë¡œ ì—…ë°ì´íŠ¸: {question[:30]}...")
            
            # ê¸°ì¡´ í”¼ë“œë°± ì •ë³´ ë³´ì¡´ (ì¹´ìš´íŠ¸, ì‚¬ìš©ì ë“±)
            old_feedback = feedbacks[same_question_index]
            feedback_count = old_feedback.get("feedback_count", 1)
            feedback_users = old_feedback.get("feedback_users", [])
            
            # ìƒˆ í”¼ë“œë°±ìœ¼ë¡œ êµì²´ (ê¸°ì¡´ ì •ë³´ ë³´ì¡´)
            feedback_data["qa_hash"] = qa_hash
            feedback_data["feedback_count"] = feedback_count
            feedback_data["first_feedback_time"] = old_feedback.get("first_feedback_time", old_feedback.get("timestamp"))
            feedback_data["last_feedback_time"] = feedback_data.get("timestamp", datetime.now().isoformat())
            feedback_data["feedback_users"] = feedback_users
            
            # ì‚¬ìš©ì ì •ë³´ ì¶”ê°€
            user = feedback_data.get("user")
            if user and user not in feedback_users:
                feedback_data["feedback_users"].append(user)
            
            feedbacks[same_question_index] = feedback_data
            
            logger.info(f"ğŸ’¾ í”¼ë“œë°± ì—…ë°ì´íŠ¸: {question[:30]}... (í”¼ë“œë°± ìˆ˜: {feedback_count})")
        else:
            # ì™„ì „íˆ ìƒˆë¡œìš´ ì§ˆë¬¸-ë‹µë³€ ì„¸íŠ¸
            feedback_data["qa_hash"] = qa_hash
            feedback_data["feedback_count"] = 1
            feedback_data["first_feedback_time"] = feedback_data.get("timestamp", datetime.now().isoformat())
            feedback_data["last_feedback_time"] = feedback_data.get("timestamp", datetime.now().isoformat())
            feedback_data["feedback_users"] = []
            
            # ì‚¬ìš©ì ì •ë³´ ì¶”ê°€
            user = feedback_data.get("user")
            if user:
                feedback_data["feedback_users"].append(user)
            
            feedbacks.append(feedback_data)
            logger.info(f"ğŸ’¾ ìƒˆë¡œìš´ í”¼ë“œë°± ì €ì¥: {question[:30]}... (ì´ {len(feedbacks)}ê°œ)")
        
        # íŒŒì¼ì— ì €ì¥
        logger.info(f"ğŸ’¾ í”¼ë“œë°± íŒŒì¼ ì €ì¥ ì‹œë„: {feedback_file} (ë°ì´í„° {len(feedbacks)}ê°œ)")
        try:
            # íŒŒì¼ ì €ì¥ ì „ ë””ë ‰í† ë¦¬ ì¬í™•ì¸
            feedback_dir = os.path.dirname(feedback_file) or DOCS_DIR
            os.makedirs(feedback_dir, exist_ok=True)
            
            # íŒŒì¼ ì €ì¥
            with open(feedback_file, 'w', encoding='utf-8') as f:
                json.dump(feedbacks, f, ensure_ascii=False, indent=2)
            
            # íŒŒì¼ì´ ì œëŒ€ë¡œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
            if not os.path.exists(feedback_file):
                raise FileNotFoundError(f"í”¼ë“œë°± íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {feedback_file}")
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            file_size = os.path.getsize(feedback_file)
            logger.info(f"âœ… í”¼ë“œë°± íŒŒì¼ ì €ì¥ ì™„ë£Œ: {feedback_file} (í¬ê¸°: {file_size} bytes)")
                
        except Exception as save_error:
            logger.error(f"âŒ í”¼ë“œë°± íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {save_error}")
            logger.error(f"âŒ ì €ì¥ ê²½ë¡œ: {feedback_file}")
            feedback_dir = os.path.dirname(feedback_file) or DOCS_DIR
            logger.error(f"âŒ ë””ë ‰í† ë¦¬ ì¡´ì¬: {os.path.exists(feedback_dir)}")
            logger.error(f"âŒ ë””ë ‰í† ë¦¬ ê²½ë¡œ: {feedback_dir}")
            import traceback
            logger.error(f"âŒ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            raise  # ì €ì¥ ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œ ìƒìœ„ì—ì„œ ì²˜ë¦¬
        
        # ê¸ì • í”¼ë“œë°±ë§Œ ë²¡í„° DB ì¬ì¸ë±ì‹±
        if feedback_type == "positive":
            index_feedback_data(feedback_file)
        
        return {
            "saved": True,
            "is_new": (same_question_index is None and existing_index is None),
            "feedback_count": feedback_data.get("feedback_count", 1)
        }
        
    except Exception as e:
        logger.error(f"âŒ í”¼ë“œë°± ì €ì¥ ì˜¤ë¥˜: {str(e)}")
        import traceback
        logger.error(f"âŒ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        return {"saved": False, "is_new": False, "feedback_count": 0}

def get_feedback_vectordb():
    """ê¸ì • í”¼ë“œë°± ë°ì´í„° ì „ìš© ë²¡í„° DB"""
    try:
        embedding = OpenAIEmbeddings()
        feedback_db_path = os.path.join(DOCS_DIR, "feedback_chroma_db")
        vectordb = Chroma(persist_directory=feedback_db_path, embedding_function=embedding)
        
        # ìë™ ì¸ë±ì‹± ë¡œì§ ì œê±° (ìˆœí™˜ ì°¸ì¡° ë°©ì§€)
        # ì¸ë±ì‹±ì€ save_feedback_to_file()ì—ì„œë§Œ ìˆ˜í–‰
        
        return vectordb
    except Exception as e:
        logger.error(f"âŒ í”¼ë“œë°± ë²¡í„° DB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None

def index_feedback_data(feedback_file):
    """ê¸ì • í”¼ë“œë°± ë°ì´í„°ë¥¼ ë²¡í„° DBì— ì¸ë±ì‹±"""
    try:
        with open(feedback_file, 'r', encoding='utf-8') as f:
            feedbacks = json.load(f)
        
        if not feedbacks:
            logger.info("ğŸ“ ì¸ë±ì‹±í•  í”¼ë“œë°± ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return
        
        docs = []
        for feedback in feedbacks:
            doc = Document(
                page_content=feedback["question"],  # ì§ˆë¬¸ì„ ë²¡í„°í™”
                metadata={
                    "answer": feedback["answer"],
                    "sources": json.dumps(feedback["sources"], ensure_ascii=False),
                    "timestamp": feedback["timestamp"],
                    "source": "positive_feedback"
                }
            )
            docs.append(doc)
        
        # ìˆœí™˜ ì°¸ì¡° ë°©ì§€: get_feedback_vectordb() ëŒ€ì‹  ì§ì ‘ ë²¡í„° DB ìƒì„±
        embedding = OpenAIEmbeddings()
        feedback_db_path = os.path.join(DOCS_DIR, "feedback_chroma_db")
        vectordb = Chroma(persist_directory=feedback_db_path, embedding_function=embedding)
        
        if docs:
            # ê¸°ì¡´ í”¼ë“œë°± ë°ì´í„° ì œê±° (ì „ì²´ ì¬ì¸ë±ì‹±)
            try:
                collection = vectordb.get()
                if collection and "ids" in collection and collection["ids"]:
                    # positive_feedback ì†ŒìŠ¤ë§Œ ì œê±°
                    ids_to_remove = []
                    if "metadatas" in collection:
                        for i, metadata in enumerate(collection["metadatas"]):
                            if isinstance(metadata, dict) and metadata.get("source") == "positive_feedback":
                                if i < len(collection["ids"]):
                                    ids_to_remove.append(collection["ids"][i])
                    
                    if ids_to_remove:
                        vectordb._collection.delete(ids_to_remove)
                        logger.info(f"ğŸ—‘ï¸ ê¸°ì¡´ í”¼ë“œë°± ë°ì´í„° ì œê±°: {len(ids_to_remove)}ê°œ")
            except Exception as del_error:
                logger.warning(f"âš ï¸ ê¸°ì¡´ í”¼ë“œë°± ë°ì´í„° ì‚­ì œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œí•˜ê³  ê³„ì†): {del_error}")
            
            # ìƒˆ ë°ì´í„° ì¶”ê°€
            vectordb.add_documents(docs)
            try:
                vectordb.persist()
            except Exception as persist_error:
                logger.warning(f"âš ï¸ persist() ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œí•˜ê³  ê³„ì†): {persist_error}")
            logger.info(f"âœ… í”¼ë“œë°± ë°ì´í„° ì¸ë±ì‹± ì™„ë£Œ: {len(docs)}ê°œ")
        
    except Exception as e:
        logger.error(f"âŒ í”¼ë“œë°± ë°ì´í„° ì¸ë±ì‹± ì˜¤ë¥˜: {str(e)}")
        import traceback
        logger.error(f"âŒ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")

def search_positive_feedback(question):
    """ê¸ì • í”¼ë“œë°± ë°ì´í„°ì—ì„œ ìœ ì‚¬ ì§ˆë¬¸ ê²€ìƒ‰
    
    JSON íŒŒì¼ì´ ì—†ìœ¼ë©´ ë²¡í„° DB ê²€ìƒ‰ì„ í•˜ì§€ ì•Šê³  Noneì„ ë°˜í™˜í•˜ì—¬ ë©”ì¸ DB ê²€ìƒ‰ìœ¼ë¡œ ë„˜ì–´ê°
    """
    try:
        # JSON íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (íŒŒì¼ ê¸°ë°˜ ê²€ìƒ‰ ë³´ì¥)
        positive_file = os.path.join(DOCS_DIR, "positive_feedback.json")
        if not os.path.exists(positive_file):
            logger.debug("ğŸ“ ê¸ì • í”¼ë“œë°± JSON íŒŒì¼ì´ ì—†ì–´ ë²¡í„° DB ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤ â†’ ë©”ì¸ DB ê²€ìƒ‰ìœ¼ë¡œ ì§„í–‰")
            return None
        
        # JSON íŒŒì¼ì´ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸ ë° ë¡œë“œ
        try:
            with open(positive_file, 'r', encoding='utf-8') as f:
                feedbacks = json.load(f)
            if not feedbacks or len(feedbacks) == 0:
                logger.debug("ğŸ“ ê¸ì • í”¼ë“œë°± JSON íŒŒì¼ì´ ë¹„ì–´ìˆì–´ ë²¡í„° DB ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤ â†’ ë©”ì¸ DB ê²€ìƒ‰ìœ¼ë¡œ ì§„í–‰")
                return None
        except (json.JSONDecodeError, Exception) as e:
            logger.warning(f"âš ï¸ ê¸ì • í”¼ë“œë°± JSON íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e} â†’ ë©”ì¸ DB ê²€ìƒ‰ìœ¼ë¡œ ì§„í–‰")
            return None
        
        # 1ë‹¨ê³„: JSON íŒŒì¼ ì§ì ‘ ê²€ìƒ‰ (ë²¡í„° DB ê²€ìƒ‰ ì „ì— ë¨¼ì € ì‹œë„)
        # í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜
        def extract_core_keywords(text, stop_words_set):
            """ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ (ê³µë°± ì œê±° + stop_words ì œê±°)"""
            if not text:
                return set()
            
            # 1. ì†Œë¬¸ì ë³€í™˜
            text_lower = text.lower()
            # 2. ë‹¨ì–´ ë¶„ë¦¬ (ì›ë³¸ì—ì„œ)
            words = text_lower.split()
            # 3. stop_words ì œê±° í›„ í•µì‹¬ ë‹¨ì–´ ì¶”ì¶œ
            core_words = [w for w in words if len(w) > 1 and w not in stop_words_set]
            
            # 4. í•µì‹¬ í‚¤ì›Œë“œë“¤ë§Œ í•©ì³ì„œ ê³µë°± ì œê±°ëœ ë¬¸ìì—´ ìƒì„±
            if core_words:
                core_no_space = "".join(core_words)
                if core_no_space and len(core_no_space) > 1:
                    core_words.append(core_no_space)
            
            return set(core_words)
        
        # stop_words ì •ì˜
        stop_words = {
            'ê³µìˆ˜', 'ì˜', 'ì—', 'ì„', 'ë¥¼', 'ì´', 'ê°€', 'ì€', 'ëŠ”', 
            'ë¡œ', 'ìœ¼ë¡œ', 'ì™€', 'ê³¼', 'ë„', 'ë§Œ', 'ê¹Œì§€', 'ë¶€í„°', 
            'ë•Œë¬¸ì—', 'ìœ„í•´', 'ëŒ€í•œ', 'ê´€ë ¨', 'ê¸°ëŠ¥', 'ê°œë°œ', 'ì‘ì—…',
            'ì•Œë ¤ì¤˜', 'ì•Œë ¤ì£¼ì„¸ìš”', 'ì•Œë ¤ì¤ì‹œë‹¤', 'ì•Œë ¤ì£¼ì‹œë©´', 'ì•Œë ¤',
            'ë¶„ì„í•´ì¤˜', 'ë¶„ì„í•´ì£¼ì„¸ìš”', 'ë¶„ì„',
            'ì–¼ë§ˆì•¼', 'ì–¼ë§ˆì˜ˆìš”', 'ì–¼ë§ˆì¸ê°€ìš”', 'ì–¼ë§ˆ',
            'ì–´ë–»ê²Œ', 'ì–´ë–¤', 'ì–´ë– í•œ',
            'ë¼', 'ë˜', 'ë˜ì–´', 'ë˜ëŠ”',
            'í•´ì¤˜', 'í•´ì£¼ì„¸ìš”', 'í•´ì£¼ì‹œë©´', 'í•´',
            'ë­ì•¼', 'ë­ì˜ˆìš”', 'ë¬´ì—‡', 'ë¬´ì—‡ì¸ê°€',
            '?', '!', '.', ',', 'ëŠ”', 'ì€', 'ì´', 'ê°€'
        }
        
        # ì¼ë°˜ì ì¸ í‚¤ì›Œë“œ (ë§¤ì¹­ ì‹œ ê°€ì¤‘ì¹˜ ê°ì†Œ)
        common_keywords = {'api', 'ì‹œìŠ¤í…œ', 'ê¸°ëŠ¥', 'ê°œë°œ', 'ì‘ì—…', 'ê°€ì´ë“œ'}
        
        # ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        question_core = extract_core_keywords(question, stop_words)
        logger.info(f"ğŸ” JSON íŒŒì¼ ì§ì ‘ ê²€ìƒ‰ - ì§ˆë¬¸: '{question}', í•µì‹¬ í‚¤ì›Œë“œ: {question_core}")
        
        # JSON íŒŒì¼ ë‚´ì—ì„œ ì§ì ‘ ê²€ìƒ‰
        best_json_match = None
        best_json_score = 0.0
        
        for feedback in feedbacks:
            stored_question = feedback.get("question", "").lower()
            stored_core = extract_core_keywords(stored_question, stop_words)
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ ë¹„ìœ¨ ê³„ì‚°
            if question_core and stored_core:
                # 1. ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­
                matched_keywords = question_core.intersection(stored_core)
                
                # 2. ë¶€ë¶„ ë¬¸ìì—´ ë§¤ì¹­
                for q_keyword in question_core:
                    for s_keyword in stored_core:
                        if q_keyword in s_keyword or s_keyword in q_keyword:
                            matched_keywords.add(q_keyword)
                            matched_keywords.add(s_keyword)
                
                # ë§¤ì¹­ ë¹„ìœ¨ ê³„ì‚°
                if len(question_core) > 0:
                    matched_question_keywords = matched_keywords.intersection(question_core)
                    
                    # ì¼ë°˜ í‚¤ì›Œë“œë§Œ ë§¤ì¹­ëœ ê²½ìš° ê°€ì¤‘ì¹˜ ê°ì†Œ
                    matched_common_only = matched_question_keywords.intersection(common_keywords)
                    matched_specific = matched_question_keywords - common_keywords
                    
                    if len(matched_specific) == 0 and len(matched_common_only) > 0:
                        keyword_match_ratio = (len(matched_question_keywords) / len(question_core)) * 0.5
                    else:
                        keyword_match_ratio = len(matched_question_keywords) / len(question_core)
                    
                    # í•µì‹¬ í‚¤ì›Œë“œê°€ ëª¨ë‘ ì¼ì¹˜í•˜ë©´ 100%ë¡œ ì²˜ë¦¬
                    if question_core.issubset(matched_keywords):
                        keyword_match_ratio = 1.0
                else:
                    keyword_match_ratio = 0.0
            else:
                keyword_match_ratio = 0.0
            
            # JSON íŒŒì¼ ì§ì ‘ ê²€ìƒ‰: ê¸°ì¤€2 - ê±°ì˜ ë™ì¼í•œ ì§ˆë¬¸ë§Œ (í‚¤ì›Œë“œ 100% ì¼ì¹˜ë§Œ í—ˆìš©)
            # ë„ì–´ì“°ê¸°, ì¡°ì‚¬ ì°¨ì´ë§Œ ìˆëŠ” ê²½ìš°ë§Œ í”¼ë“œë°± ë‹µë³€ ì‚¬ìš©
            is_match = False
            if keyword_match_ratio >= 1.0:
                # í•µì‹¬ í‚¤ì›Œë“œê°€ ëª¨ë‘ ì¼ì¹˜í•˜ë©´ ë¬´ì¡°ê±´ ë§¤ì¹­ (ê¸°ì¤€2: ê±°ì˜ ë™ì¼í•œ ì§ˆë¬¸)
                is_match = True
                logger.info(f"   âœ… JSON íŒŒì¼ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ 100% ì¼ì¹˜ ë°œê²¬ (ê¸°ì¤€2): '{stored_question[:50]}...'")
            
            if is_match and keyword_match_ratio > best_json_score:
                best_json_score = keyword_match_ratio
                best_json_match = feedback
        
        # JSON íŒŒì¼ì—ì„œ ë§¤ì¹­ëœ í•­ëª©ì´ ìˆìœ¼ë©´ ë°”ë¡œ ë°˜í™˜
        if best_json_match:
            logger.info(f"âœ… JSON íŒŒì¼ì—ì„œ ì§ì ‘ ë‹µë³€ ë°œê²¬ (í‚¤ì›Œë“œë§¤ì¹­={best_json_score:.3f})")
            return {
                "answer": best_json_match.get("answer", ""),
                "sources": best_json_match.get("sources", []),
                "question": best_json_match.get("question", ""),
                "is_from_feedback": True
            }
        
        # 2ë‹¨ê³„: JSON íŒŒì¼ì—ì„œ ë§¤ì¹­ë˜ì§€ ì•Šìœ¼ë©´ ë²¡í„° DBì—ì„œ ê²€ìƒ‰ (ê¸°ì¡´ ë¡œì§)
        logger.debug("ğŸ“ JSON íŒŒì¼ì—ì„œ ì§ì ‘ ë§¤ì¹­ë˜ì§€ ì•ŠìŒ, ë²¡í„° DB ê²€ìƒ‰ìœ¼ë¡œ ì§„í–‰")
        
        # ë²¡í„° DBì—ì„œ ê²€ìƒ‰ (JSON íŒŒì¼ì´ ì¡´ì¬í•˜ê³  ë¹„ì–´ìˆì§€ ì•Šì„ ë•Œë§Œ)
        # ë²¡í„° DB ì´ˆê¸°í™”ëŠ” í•œ ë²ˆë§Œ ìˆ˜í–‰í•˜ë„ë¡ ìµœì í™” (ì „ì—­ ë³€ìˆ˜ ë˜ëŠ” ìºì‹± ê³ ë ¤)
        try:
            # ë²¡í„° DB ì´ˆê¸°í™” (ë§¤ë²ˆ í˜¸ì¶œë˜ì§€ë§Œ ChromaëŠ” ë‚´ë¶€ì ìœ¼ë¡œ ìµœì í™”ë¨)
            feedback_vectordb = get_feedback_vectordb()
            if not feedback_vectordb:
                logger.debug("ğŸ“ í”¼ë“œë°± ë²¡í„° DB ì´ˆê¸°í™” ì‹¤íŒ¨ â†’ ë©”ì¸ DB ê²€ìƒ‰ìœ¼ë¡œ ì§„í–‰")
                return None
            
            # ë²¡í„° DBì— ë¬¸ì„œê°€ ìˆëŠ”ì§€ ë¹ ë¥´ê²Œ í™•ì¸
            try:
                doc_count = feedback_vectordb._collection.count()
                if doc_count == 0:
                    logger.debug("ğŸ“ í”¼ë“œë°± ë²¡í„° DBì— ë¬¸ì„œê°€ ì—†ìŒ â†’ ë©”ì¸ DB ê²€ìƒ‰ìœ¼ë¡œ ì§„í–‰")
                    return None
            except Exception as count_error:
                logger.debug(f"âš ï¸ í”¼ë“œë°± ë²¡í„° DB ë¬¸ì„œ ìˆ˜ í™•ì¸ ì‹¤íŒ¨: {count_error}, ê²€ìƒ‰ ê³„ì† ì§„í–‰")
            
            # í”¼ë“œë°± ê²€ìƒ‰ ìµœì í™”: similarity_search_with_scoreë§Œ ì‚¬ìš© (MMR ìƒëµ)
            # í”¼ë“œë°± ë°ì´í„°ëŠ” ì ìœ¼ë¯€ë¡œ ë¹ ë¥¸ ê²€ìƒ‰ì´ ì¤‘ìš”
            # ë„ì–´ì“°ê¸° ì°¨ì´ë¥¼ ê³ ë ¤í•˜ì—¬ ê³µë°± ì œê±° ë²„ì „ë„ ê²€ìƒ‰
            try:
                # ì›ë³¸ ì§ˆë¬¸ìœ¼ë¡œ ê²€ìƒ‰
                scored_docs = feedback_vectordb.similarity_search_with_score(question, k=5)
                
                # ê³µë°± ì œê±° ë²„ì „ìœ¼ë¡œë„ ê²€ìƒ‰ (ë„ì–´ì“°ê¸° ì°¨ì´ ëŒ€ì‘)
                question_no_space = question.replace(" ", "")
                if question_no_space != question:
                    scored_docs_no_space = feedback_vectordb.similarity_search_with_score(question_no_space, k=5)
                    # ë‘ ê²°ê³¼ë¥¼ í•©ì¹˜ê³  ì¤‘ë³µ ì œê±° (ê±°ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬)
                    all_docs = {}
                    for doc, score in scored_docs:
                        doc_key = doc.page_content
                        if doc_key not in all_docs or score < all_docs[doc_key][1]:
                            all_docs[doc_key] = (doc, score)
                    for doc, score in scored_docs_no_space:
                        doc_key = doc.page_content
                        if doc_key not in all_docs or score < all_docs[doc_key][1]:
                            all_docs[doc_key] = (doc, score)
                    # ê±°ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ 5ê°œ ì„ íƒ
                    scored_docs = sorted(all_docs.values(), key=lambda x: x[1])[:5]
                
                if not scored_docs:
                    logger.debug("ğŸ“ í”¼ë“œë°± ë²¡í„° DBì—ì„œ ìœ ì‚¬ ì§ˆë¬¸ì„ ì°¾ì§€ ëª»í•¨ â†’ ë©”ì¸ DB ê²€ìƒ‰ìœ¼ë¡œ ì§„í–‰")
                    return None
                
                docs_with_scores = [(doc, score) for doc, score in scored_docs]
            except (AttributeError, Exception) as e:
                # similarity_search_with_scoreê°€ ì—†ìœ¼ë©´ MMRë¡œ í´ë°±
                logger.debug(f"âš ï¸ similarity_search_with_score ì‹¤íŒ¨, MMRë¡œ í´ë°±: {e}")
                retriever = feedback_vectordb.as_retriever(
                    search_type="mmr",
                    search_kwargs={"k": 3, "fetch_k": 10}
                )
                docs = retriever.get_relevant_documents(question)
                if not docs:
                    logger.debug("ğŸ“ í”¼ë“œë°± ë²¡í„° DBì—ì„œ ìœ ì‚¬ ì§ˆë¬¸ì„ ì°¾ì§€ ëª»í•¨ â†’ ë©”ì¸ DB ê²€ìƒ‰ìœ¼ë¡œ ì§„í–‰")
                    return None
                docs_with_scores = [(doc, 0.0) for doc in docs]
            
            if not docs_with_scores:
                logger.debug("ğŸ“ í”¼ë“œë°± ë²¡í„° DBì—ì„œ ìœ ì‚¬ ì§ˆë¬¸ì„ ì°¾ì§€ ëª»í•¨ â†’ ë©”ì¸ DB ê²€ìƒ‰ìœ¼ë¡œ ì§„í–‰")
                return None
            
            # ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (ìœ„ì—ì„œ ì´ë¯¸ ì¶”ì¶œí–ˆì§€ë§Œ ë²¡í„° DB ê²€ìƒ‰ì„ ìœ„í•´ ì¬ì‚¬ìš©)
            logger.info(f"ğŸ” ë²¡í„° DB ê²€ìƒ‰ - ì§ˆë¬¸: '{question}', í•µì‹¬ í‚¤ì›Œë“œ: {question_core}")
            
            # ìœ ì‚¬ë„ ì ìˆ˜ì™€ í‚¤ì›Œë“œ ë§¤ì¹­ì„ ê³ ë ¤í•˜ì—¬ ìµœì ì˜ ê²°ê³¼ ì„ íƒ
            best_match = None
            best_score = 0.0
            
            # ê¸°ì¤€2: ê±°ì˜ ë™ì¼í•œ ì§ˆë¬¸ë§Œ í”¼ë“œë°± ë‹µë³€ ì‚¬ìš©
            # ë²¡í„° DB ê²€ìƒ‰ì—ì„œëŠ” ê±°ë¦¬ < 0.1 + í‚¤ì›Œë“œ 100% ì¼ì¹˜ë§Œ í—ˆìš©
            # (ê¸°ì¡´ 0.15ë³´ë‹¤ ë” ì—„ê²©í•˜ê²Œ ì¡°ì •)
            
            # ìœ ì‚¬ë„ê°€ ë†’ì§€ ì•Šìœ¼ë©´ í‚¤ì›Œë“œ ë§¤ì¹­ ìˆ˜í–‰
            for doc, score in docs_with_scores:
                # ìœ ì‚¬ë„ ì ìˆ˜ í™•ì¸ (Chroma DBëŠ” ê±°ë¦¬ ê¸°ë°˜ì´ë¯€ë¡œ ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬í•¨)
                # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜ (0~1 ë²”ìœ„, 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬)
                # ì¼ë°˜ì ìœ¼ë¡œ ê±°ë¦¬ 1.0 ì´í•˜ë¥¼ ìœ ì‚¬í•˜ë‹¤ê³  ë´„
                similarity = 1.0 / (1.0 + score) if score > 0 else 1.0
                
                # ì €ì¥ëœ ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
                stored_question = doc.page_content.lower()
                stored_core = extract_core_keywords(stored_question, stop_words)
                logger.debug(f"ğŸ” í”¼ë“œë°± ë§¤ì¹­ - ì €ì¥ëœ ì§ˆë¬¸: '{stored_question}', í•µì‹¬ í‚¤ì›Œë“œ: {stored_core}")
                
                # í‚¤ì›Œë“œ ë§¤ì¹­ ë¹„ìœ¨ ê³„ì‚°
                if question_core and stored_core:
                    # 1. ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­
                    matched_keywords = question_core.intersection(stored_core)
                    
                    # 2. ë¶€ë¶„ ë¬¸ìì—´ ë§¤ì¹­ (ê³µë°± ì œê±°ëœ ì „ì²´ ë¬¸ìì—´ í¬í•¨)
                    # ì €ì¥ëœ ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œê°€ ê²€ìƒ‰ ì§ˆë¬¸ì— í¬í•¨ë˜ê±°ë‚˜ ê·¸ ë°˜ëŒ€ì¸ ê²½ìš°
                    for q_keyword in question_core:
                        for s_keyword in stored_core:
                            # ë¶€ë¶„ ë¬¸ìì—´ ë§¤ì¹­ (ì–‘ë°©í–¥)
                            if q_keyword in s_keyword or s_keyword in q_keyword:
                                matched_keywords.add(q_keyword)
                                matched_keywords.add(s_keyword)
                    
                    # ë§¤ì¹­ ë¹„ìœ¨ ê³„ì‚°: ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œ ì¤‘ ë§¤ì¹­ëœ ë¹„ìœ¨ (ë” ì—„ê²©í•œ ê¸°ì¤€)
                    # ì¼ë°˜ í‚¤ì›Œë“œë§Œ ë§¤ì¹­ëœ ê²½ìš° ê°€ì¤‘ì¹˜ ê°ì†Œ
                    if len(question_core) > 0:
                        # ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œ ì¤‘ ë§¤ì¹­ëœ ë¹„ìœ¨
                        matched_question_keywords = matched_keywords.intersection(question_core)
                        
                        # ì¼ë°˜ í‚¤ì›Œë“œë§Œ ë§¤ì¹­ëœ ê²½ìš° ê°€ì¤‘ì¹˜ ê°ì†Œ
                        matched_common_only = matched_question_keywords.intersection(common_keywords)
                        matched_specific = matched_question_keywords - common_keywords
                        
                        # ì¼ë°˜ í‚¤ì›Œë“œë§Œ ìˆê³  íŠ¹ì • í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ê°€ì¤‘ì¹˜ ê°ì†Œ
                        if len(matched_specific) == 0 and len(matched_common_only) > 0:
                            # ì¼ë°˜ í‚¤ì›Œë“œë§Œ ë§¤ì¹­ëœ ê²½ìš° ê°€ì¤‘ì¹˜ 50% ê°ì†Œ
                            keyword_match_ratio = (len(matched_question_keywords) / len(question_core)) * 0.5
                            logger.info(f"   âš ï¸ ì¼ë°˜ í‚¤ì›Œë“œë§Œ ë§¤ì¹­ë¨ (ê°€ì¤‘ì¹˜ 50% ê°ì†Œ): {matched_common_only}")
                        else:
                            keyword_match_ratio = len(matched_question_keywords) / len(question_core)
                    else:
                        keyword_match_ratio = 0.0
                    
                    # í•µì‹¬ í‚¤ì›Œë“œê°€ ëª¨ë‘ ì¼ì¹˜í•˜ë©´ í‚¤ì›Œë“œ ë§¤ì¹­ì„ 100%ë¡œ ì²˜ë¦¬
                    # ë‹¨, ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œê°€ ëª¨ë‘ ë§¤ì¹­ë˜ì–´ì•¼ í•¨
                    if len(question_core) > 0 and question_core.issubset(matched_keywords):
                        keyword_match_ratio = 1.0
                else:
                    keyword_match_ratio = 0.0
                    matched_keywords = set()
                
                # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ìœ ì‚¬ë„ 60% + í‚¤ì›Œë“œ ë§¤ì¹­ 40%)
                # í‚¤ì›Œë“œ ë§¤ì¹­ì´ ë†’ìœ¼ë©´ ìœ ì‚¬ë„ê°€ ë‚®ì•„ë„ í—ˆìš©
                combined_score = (similarity * 0.6) + (keyword_match_ratio * 0.4)
                
                logger.info(f"ğŸ” í”¼ë“œë°± ë§¤ì¹­ ì ìˆ˜: ìœ ì‚¬ë„={similarity:.3f} (ê±°ë¦¬={score:.3f}), í‚¤ì›Œë“œë§¤ì¹­={keyword_match_ratio:.3f}, ì¢…í•©={combined_score:.3f}")
                logger.info(f"   ì§ˆë¬¸: '{question}' (í•µì‹¬í‚¤ì›Œë“œ: {question_core}) vs ì €ì¥ëœ: '{doc.page_content[:50]}...' (í•µì‹¬í‚¤ì›Œë“œ: {stored_core})")
                
                # ê¸°ì¤€2: ê±°ì˜ ë™ì¼í•œ ì§ˆë¬¸ë§Œ í”¼ë“œë°± ë‹µë³€ ì‚¬ìš©
                # ë²¡í„° DB ê²€ìƒ‰ì—ì„œëŠ” ê±°ë¦¬ < 0.1 + í‚¤ì›Œë“œ 100% ì¼ì¹˜ë§Œ í—ˆìš©
                is_match = False
                if keyword_match_ratio >= 1.0 and score < 0.1:
                    # í•µì‹¬ í‚¤ì›Œë“œê°€ ëª¨ë‘ ì¼ì¹˜í•˜ê³  ê±°ë¦¬ê°€ ë§¤ìš° ê°€ê¹Œìš¸ ë•Œë§Œ ë§¤ì¹­ (ê¸°ì¤€2)
                    is_match = True
                    logger.info(f"   âœ… ê¸°ì¤€2: í•µì‹¬ í‚¤ì›Œë“œ 100% ì¼ì¹˜ + ê±°ë¦¬ {score:.3f} < 0.1ë¡œ ë§¤ì¹­")
                
                if is_match:
                    if combined_score > best_score:
                        best_score = combined_score
                        best_match = (doc, similarity, keyword_match_ratio)
            
            if best_match:
                doc, similarity, keyword_match_ratio = best_match
                logger.info(f"âœ… í”¼ë“œë°± ë²¡í„° DBì—ì„œ ë‹µë³€ ë°œê²¬ (ìœ ì‚¬ë„={similarity:.3f}, í‚¤ì›Œë“œë§¤ì¹­={keyword_match_ratio:.3f})")
                return {
                    "answer": doc.metadata["answer"],
                    "sources": json.loads(doc.metadata["sources"]),
                    "question": doc.page_content,
                    "is_from_feedback": True
                }
            else:
                logger.debug(f"ğŸ“ í”¼ë“œë°± ë²¡í„° DBì—ì„œ ì„ê³„ê°’ì„ ë§Œì¡±í•˜ëŠ” ì§ˆë¬¸ì„ ì°¾ì§€ ëª»í•¨ (ìµœê³ ì ìˆ˜={best_score:.3f}) â†’ ë©”ì¸ DB ê²€ìƒ‰ìœ¼ë¡œ ì§„í–‰")
                return None
            
        except Exception as vectordb_error:
            # ë²¡í„° DB ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ë©”ì¸ DB ê²€ìƒ‰ìœ¼ë¡œ ë„˜ì–´ê°€ë„ë¡ None ë°˜í™˜
            logger.warning(f"âš ï¸ í”¼ë“œë°± ë²¡í„° DB ê²€ìƒ‰ ì˜¤ë¥˜: {vectordb_error} â†’ ë©”ì¸ DB ê²€ìƒ‰ìœ¼ë¡œ ì§„í–‰")
            return None
        
    except Exception as e:
        # ëª¨ë“  ì˜¤ë¥˜ë¥¼ ì¡ì•„ì„œ ë©”ì¸ DB ê²€ìƒ‰ìœ¼ë¡œ ë„˜ì–´ê°€ë„ë¡ None ë°˜í™˜
        logger.warning(f"âš ï¸ í”¼ë“œë°± ê²€ìƒ‰ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)} â†’ ë©”ì¸ DB ê²€ìƒ‰ìœ¼ë¡œ ì§„í–‰")
        return None

def index_json_data_incremental(jira_tickets: list, file_path: str = None):
    """íŠ¹ì • Jira í‹°ì¼“ë“¤ë§Œ ì¦ë¶„ ìƒ‰ì¸ (ì¶”ê°€/ìˆ˜ì •)"""
    try:
        if not file_path:
            file_path = os.path.join(DOCS_DIR, "effort_estimations.json")
        
        if not os.path.exists(file_path):
            logger.warning(f"âš ï¸ JSON íŒŒì¼ ì—†ìŒ: {file_path}")
            return False
        
        # ë²¡í„° DB ìƒì„±
        embedding = OpenAIEmbeddings()
        vectordb = Chroma(persist_directory=CHROMA_DIR, embedding_function=embedding)
        
        # JSON íŒŒì¼ ì½ê¸°
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # ëŒ€ìƒ í‹°ì¼“ë§Œ í•„í„°ë§
        target_items = [item for item in data if item.get('jira_ticket') in jira_tickets]
        
        if not target_items:
            logger.info(f"ğŸ“Š ì¦ë¶„ ìƒ‰ì¸: ëŒ€ìƒ í•­ëª© ì—†ìŒ")
            return True
        
        logger.info(f"ğŸ“Š ì¦ë¶„ ìƒ‰ì¸: {len(target_items)}ê°œ í•­ëª© ì²˜ë¦¬ ì¤‘...")
        
        # ê¸°ì¡´ ë°ì´í„° ì œê±° (í•´ë‹¹ í‹°ì¼“ë§Œ) - ìµœì í™”: where í•„í„° ì‚¬ìš©
        try:
            # Chroma where í•„í„°ë¡œ íŠ¹ì • í‹°ì¼“ë§Œ ì¡°íšŒ (ì „ì²´ DB ìˆœíšŒ ì—†ìŒ)
            collection = vectordb.get(where={"jira_ticket": {"$in": jira_tickets}})
            docs_to_remove_ids = collection.get("ids", [])
            
            if docs_to_remove_ids:
                vectordb._collection.delete(docs_to_remove_ids)
                logger.info(f"   ğŸ—‘ï¸ ê¸°ì¡´ ë°ì´í„° ì œê±°: {len(docs_to_remove_ids)}ê°œ")
        except Exception as del_error:
            logger.warning(f"âš ï¸ ê¸°ì¡´ ë°ì´í„° ì œê±° ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œí•˜ê³  ê³„ì†): {del_error}")
        
        # ìƒˆ ë°ì´í„° ìƒ‰ì¸
        docs = []
        for item in target_items:
            # Epic ì •ë³´
            epic_info = ""
            if item.get('epic_key'):
                epic_info = f"\nEpic: {item.get('epic_key', '')}"
                if item.get('epic_name'):
                    epic_info += f" ({item.get('epic_name', '')})"
            
            # Story Points í‘œì‹œ (ì›ë³¸ ì •ë³´ í¬í•¨)
            story_points_display = f"{item.get('story_points', '')} M/D"
            if item.get('story_points_unit') == 'M/M':
                story_points_display += f" (ì›ë³¸: {item.get('story_points_original', '')} M/M)"
            
            # í…ìŠ¤íŠ¸ ìƒì„±
            text_content = f"""
Jira í‹°ì¼“: {item.get('jira_ticket', '')}
ì œëª©: {item.get('title', '')}{epic_info}
Story Points: {story_points_display}
ë‹´ë‹¹ì: {item.get('team_member', '')}
ì‚°ì • ì´ìœ : {item.get('estimation_reason', '')}
ì„¤ëª…: {item.get('description', '')}
ëŒ“ê¸€: {item.get('comments', '')}
ë¹„ê³ : {item.get('notes', '')}
ë“±ë¡ì¼: {item.get('created_date', '')}
"""
            
            doc = Document(
                page_content=text_content.strip(),
                metadata={
                    "source": "effort_estimations.json",
                    "jira_ticket": item.get('jira_ticket', ''),
                    "title": item.get('title', ''),
                    "story_points": item.get('story_points', ''),
                    "story_points_original": item.get('story_points_original', ''),
                    "story_points_unit": item.get('story_points_unit', 'M/D'),
                    "team_member": item.get('team_member', ''),
                    "major_category": item.get('major_category', ''),
                    "minor_category": item.get('minor_category', ''),
                    "sub_category": item.get('sub_category', ''),
                    "epic_key": item.get('epic_key', ''),
                    "epic_name": item.get('epic_name', ''),
                    "last_modified": datetime.fromtimestamp(os.path.getmtime(file_path)).isoformat(),
                    "file_size": os.path.getsize(file_path)
                }
            )
            docs.append(doc)
        
        # ë²¡í„° DBì— ì¶”ê°€
        if docs:
            vectordb.add_documents(docs)
            try:
                vectordb.persist()
            except Exception:
                pass  # persist() ë©”ì„œë“œê°€ ì—†ì„ ìˆ˜ ìˆìŒ
            logger.info(f"   âœ… ì¦ë¶„ ìƒ‰ì¸ ì™„ë£Œ: {len(docs)}ê°œ ì¶”ê°€")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ì¦ë¶„ ìƒ‰ì¸ ì‹¤íŒ¨: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return False

def index_json_data(file_path: str, force: bool = False):
    """JSON íŒŒì¼ì„ ë²¡í„° DBì— ì¸ë±ì‹± (ì „ì²´ ì¬ìƒ‰ì¸)"""
    try:
        # ì§ì ‘ ë²¡í„° DB ìƒì„± (get_vectordb() í˜¸ì¶œí•˜ì§€ ì•ŠìŒ)
        embedding = OpenAIEmbeddings()
        vectordb = Chroma(persist_directory=CHROMA_DIR, embedding_function=embedding)
        
        # ê¸°ì¡´ JSON ë°ì´í„° ì œê±°
        try:
            collection = vectordb.get()
            docs_to_remove = []
            if collection and "metadatas" in collection and "ids" in collection:
                for i, metadata in enumerate(collection["metadatas"]):
                    if isinstance(metadata, dict) and metadata.get("source") == "effort_estimations.json":
                        if i < len(collection["ids"]):
                            docs_to_remove.append(collection["ids"][i])
            
            if docs_to_remove:
                vectordb._collection.delete(docs_to_remove)
                logger.info(f"ğŸ—‘ï¸ Removed old JSON data: {len(docs_to_remove)} documents")
        except Exception as del_error:
            logger.warning(f"âš ï¸ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œí•˜ê³  ê³„ì†): {del_error}")
        
        # JSON íŒŒì¼ ì½ê¸°
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        logger.info(f"ğŸ“Š JSON íŒŒì¼ì—ì„œ {len(data)}ê°œ í•­ëª©ì„ ì½ì—ˆìŠµë‹ˆë‹¤")
        
        # JSON ë°ì´í„°ë¥¼ Documentë¡œ ë³€í™˜
        docs = []
        for item in data:
            # JSON ë°ì´í„°ë¥¼ ê²€ìƒ‰ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            epic_info = ""
            if item.get('epic_key'):
                epic_info = f"\nEpic: {item.get('epic_key', '')}"
                if item.get('epic_name'):
                    epic_info += f" ({item.get('epic_name', '')})"
            
            # Story Points í‘œì‹œ (ì›ë³¸ ì •ë³´ í¬í•¨)
            story_points_display = f"{item.get('story_points', '')} M/D"
            if item.get('story_points_unit') == 'M/M':
                story_points_display += f" (ì›ë³¸: {item.get('story_points_original', '')} M/M)"
            
            text_content = f"""
Jira í‹°ì¼“: {item.get('jira_ticket', '')}
ì œëª©: {item.get('title', '')}{epic_info}
Story Points: {story_points_display}
ë‹´ë‹¹ì: {item.get('team_member', '')}
ì‚°ì • ì´ìœ : {item.get('estimation_reason', '')}
ì„¤ëª…: {item.get('description', '')}
ëŒ“ê¸€: {item.get('comments', '')}
ë¹„ê³ : {item.get('notes', '')}
ë“±ë¡ì¼: {item.get('created_date', '')}
"""
            
            doc = Document(
                page_content=text_content.strip(),
                metadata={
                    "source": "effort_estimations.json",
                    "jira_ticket": item.get('jira_ticket', ''),
                    "title": item.get('title', ''),
                    "story_points": item.get('story_points', ''),
                    "story_points_original": item.get('story_points_original', ''),
                    "story_points_unit": item.get('story_points_unit', 'M/D'),
                    "team_member": item.get('team_member', ''),
                    "major_category": item.get('major_category', ''),
                    "minor_category": item.get('minor_category', ''),
                    "sub_category": item.get('sub_category', ''),
                    "epic_key": item.get('epic_key', ''),
                    "epic_name": item.get('epic_name', ''),
                    "last_modified": datetime.fromtimestamp(os.path.getmtime(file_path)).isoformat(),
                    "file_size": os.path.getsize(file_path)
                }
            )
            docs.append(doc)
        
        logger.info(f"ğŸ“Š ì´ {len(docs)}ê°œ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤")
        
        # ë°°ì¹˜ë³„ë¡œ ë¬¸ì„œ ì¶”ê°€
        BATCH_SIZE = 10
        total_added = 0
        for i in range(0, len(docs), BATCH_SIZE):
            try:
                batch = docs[i:i + BATCH_SIZE]
                vectordb.add_documents(batch)
                # persist()ëŠ” ì„ íƒì ì´ë¯€ë¡œ ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ê³„ì† ì§„í–‰
                try:
                    vectordb.persist()
                except Exception as persist_error:
                    logger.warning(f"âš ï¸ persist() ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œí•˜ê³  ê³„ì†): {persist_error}")
                total_added += len(batch)
                logger.info(f"âœ… Processed batch {i//BATCH_SIZE + 1} of {(len(docs)-1)//BATCH_SIZE + 1} (ì´ {total_added}ê°œ ë¬¸ì„œ ì¶”ê°€ë¨)")
            except Exception as batch_error:
                logger.error(f"âŒ ë°°ì¹˜ {i//BATCH_SIZE + 1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {batch_error}")
                # ì¼ë¶€ ë°°ì¹˜ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                continue
        
        logger.info(f"âœ… JSON ë°ì´í„° ì¸ë±ì‹± ì™„ë£Œ: {file_path} (ì´ {total_added}ê°œ ë¬¸ì„œ ì €ì¥ë¨)")
        return True
        
    except Exception as e:
        logger.error(f"âŒ JSON ë°ì´í„° ì¸ë±ì‹± ì˜¤ë¥˜: {str(e)}")
        import traceback
        logger.error(f"âŒ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        return False